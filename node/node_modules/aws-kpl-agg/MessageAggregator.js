'use strict';

var debug = true;

var ProtoBuf = require("protobufjs");
var crypto = require("crypto");
var _ = require('underscore');
var libPath = ".";
var common = require('./common');
require('./constants');
var magic = new Buffer(kplConfig[useKplVersion].magicNumber, 'hex');
var AggregatedRecord;

// calculate the maximum amount of data to accumulate before emitting to
// kinesis. 1MB - 16 bytes for checksum and the length of the magic number
var KINESIS_MAX_PAYLOAD_BYTES = 1048576 - 16 - Buffer.byteLength(magic);

module.exports = MessageAggregator;

function calculateVarIntSize(value) {
	if (value < 0) {
		raise
		Error("Size values should not be negative.");
	} else if (value == 0) {
		return 1;
	}

	var numBitsNeeded = 0;

	// shift the value right one bit at a time until
	// there are no more '1' bits left...this should count
	// how many bits we need to represent the number
	while (value > 0) {
		numBitsNeeded++;
		value = value >> 1;
	}

	// varints are 2's complement, so we need to add one at
	// the end so the value won't start with 1 (and thereby be negative)
	numBitsNeeded++;

	// varints only use 7 bits of the byte for the actual value
	var numVarintBytes = numBitsNeeded / 7;
	if (numBitsNeeded % 7 > 0) {
		numVarintBytes += 1;
	}

	return numVarintBytes;
}

// function to compute the required hash key form a set of records
function getHashKey(records) {
	if (debug)
		console.log("Calculating Hash Key from " + records.length + " Records");

	// calculate the explicit hash key
	// as the first record's ExcplicitHashKey or PartitionKey
	return records[0].ExplicitHashKey ? records[0].ExplicitHashKey
			: records[0].PartitionKey;
};

// function which encodes the provided records
function flush(records, onReadyCallback) {
	if (debug)
		console.log("Flushing " + records.length + " records.");

	var hashKey = getHashKey(records);

	aggregate(records, function(err, encoded) {
		if (err) {
			onReadyCallback(err);
		} else {

			// call the provided callback
			onReadyCallback(undefined, {
				PartitionKey : 'a',
				ExplicitHashKey : hashKey,
				Data : encoded
			});
		}
	});
};

function getSizeIncrement(value) {
	var newBytes = 0;
	if (value) {
		newBytes += 1; // (message index + wire type for index table)
		newBytes += calculateVarIntSize(value.length); // size of value
		// length value
		newBytes += value.length; // actual value length
	}

	return newBytes;
};

function aggregate(records, callback) {
	if (debug)
		console.log("Protobuf Aggregation of " + records.length + " records");

	try {
		if (!AggregatedRecord) {
			AggregatedRecord = common.loadBuilder();
		}

		var partitionKeyTable = {};
		var partitionKeyCount = 0;
		var explicitHashKeyTable = {};
		var explicitHashKeyCount = 0;
		var putRecords = [];

		records
				.map(function(record) {
					// add the partition key and explicit hash key
					// entries
					if (!partitionKeyTable.hasOwnProperty(record.PartitionKey)) {
						partitionKeyTable[record.PartitionKey] = partitionKeyCount;
						partitionKeyCount += 1;
					}
					if (!explicitHashKeyTable
							.hasOwnProperty(record.ExplicitHashKey)) {
						explicitHashKeyTable[record.ExplicitHashKey] = explicitHashKeyCount;
						explicitHashKeyCount += 1;
					}

					// add the AggregatedRecord object with partition
					// and hash
					// key indexes
					putRecords
							.push({
								"partition_key_index" : partitionKeyTable[record.PartitionKey],
								"explicit_hash_key_index" : explicitHashKeyTable[record.ExplicitHashKey],
								data : record.Data,
								tags : []
							});
				});

		// encode the data
		var protoData = AggregatedRecord.encode({
			"partition_key_table" : Object.keys(partitionKeyTable),
			"explicit_hash_key_table" : Object.keys(explicitHashKeyTable),
			"records" : putRecords
		});

		if (debug) {
			var debugRecords = [];
			putRecords.map(function(record) {
				debugRecords.push(record.data.toString('base64'));
			})
			console.log(JSON.stringify({
				"partition_key_table" : Object.keys(partitionKeyTable),
				"explicit_hash_key_table" : Object.keys(explicitHashKeyTable),
				records : debugRecords
			}));
		}

		// get the md5 for the encoded data
		var md5 = crypto.createHash('md5');
		md5.update(protoData.toBuffer());
		var checksum = md5.digest();

		// create the final object as a concatenation of the magic KPL number,
		// the encoded data records, and the md5 checksum
		var f = Buffer.concat([ magic, protoData.toBuffer(), checksum ]);

		if (debug) {
			console.log("Checksum: " + checksum.toString('base64'));
			console
					.log("Encoding complete. Created Protobuf AggregatedRecord of size "
							+ Buffer.byteLength(f.toString('base64')))
		}
		// call the provided callback with no-error and the final value as
		// base64 encoded string
		callback(null, f.toString('base64'));
	} catch (e) {
		console.log(e);
		callback(e);
	}
};
module.exports.aggregate = aggregate;

// constructor
function MessageAggregator(records) {
	if (!AggregatedRecord) {
		AggregatedRecord = common.loadBuilder();
	}

	this.totalBytes = 0;
	this.putRecords = [];
	this.partitionKeyTable = {};
	this.partitionKeyCount = 0;
	this.explicitHashKeyTable = {};
	this.explicitHashKeyCount = 0;

	// initialise the current state with the provided records
	if (records) {
		this.totalBytes = calculateSize(records);
		this.putRecords.push(records);
	}
};

// method to force a flush of the current inflight records
MessageAggregator.prototype.flushBufferedRecords = function(onReadyCallback) {
	flush(this.putRecords, onReadyCallback);
};

// method to aggregate a set of records
MessageAggregator.prototype.aggregateRecords = function(records, forceFlush,
		afterRecordsCallback, onReadyCallback) {
	var self = this;

	records
			.map(function(record) {
				var newBytes = 0;

				// calculate the total new message size when aggregated into
				// protobuf
				if (!self.partitionKeyTable.hasOwnProperty(record.PartitionKey)) {
					self.partitionKeyTable[record.PartitionKey] = self.partitionKeyCount;
					self.partitionKeyCount += 1;
				}

				// add the size of the partition key when encoded
				newBytes += getSizeIncrement(record.PartitionKey);

				if (!self.explicitHashKeyTable
						.hasOwnProperty(record.ExplicitHashKey)) {
					self.explicitHashKeyTable[record.ExplicitHashKey] = self.explicitHashKeyCount;
					self.explicitHashKeyCount += 1;
				}

				// add the size of the explicit hash key when encoded
				if (record.ExplicitHashKey) {
					newBytes += getSizeIncrement(record.ExplicitHashKey);
				}

				/* compute the data record length */
				var recordSize = 1;

				// add the sizes of the partition and hash key indexes
				recordSize += calculateVarIntSize(self.partitionKeyCount);
				recordSize += calculateVarIntSize(self.explicitHashKeyCount);

				var dataLength = Buffer.byteLength(new Buffer(record.Data,
						'base64'));

				// message index + wire type for record
				// data
				recordSize += 1;
				// size of data length value
				recordSize += calculateVarIntSize(dataLength);
				recordSize += dataLength; // actual data length

				// message index + wire type for record
				newBytes += 1;
				// size of entire record length value
				newBytes += calculateVarIntSize(recordSize);
				// actual entire record length
				newBytes += recordSize;

				if (debug) {
					console.log("Current Pending Size: "
							+ self.putRecords.length + " records, "
							+ self.totalBytes + " bytes");
					console.log("Next: " + newBytes + " bytes");
				}

				// if the size of this record would push us over the limit,
				// then encode the current set
				if ((self.totalBytes + newBytes) > KINESIS_MAX_PAYLOAD_BYTES
						|| forceFlush === true) {
					// flush with a copy of the current inflight records
					flush(_.clone(self.putRecords), onReadyCallback);

					// total size tracked is now the size of the current record
					self.totalBytes = newBytes;

					// current inflight becomes just this record
					self.putRecords = [ record ];
				} else {
					// the current set of records is still within the kinesis
					// max
					// payload size so increment inflight/total bytes
					self.putRecords.push(record);
					self.totalBytes = self.totalBytes + newBytes;
				}
			});

	// done - call the afterRecordsCallback
	if (afterRecordsCallback)
		afterRecordsCallback();
};