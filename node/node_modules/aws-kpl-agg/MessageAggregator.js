var debug = true;

var ProtoBuf = require("protobufjs");
var crypto = require("crypto");
var _ = require('underscore');
var libPath = ".";
require('./constants');
var magic = new Buffer(kplConfig[useKplVersion].magicNumber, 'hex');
var AggregatedRecord;
var aggregationUtils = require(libPath + "/" + 'kpl-agg')

// calculate the maximum amount of data to accumulate before emitting to
// kinesis. 1MB - 16 bytes for checksum and the length of the magic number
var KINESIS_MAX_PAYLOAD_BYTES = 1048576 - 16 - Buffer.byteLength(magic);

// function to load the protobuf configuration
var loadBuilder = function() {
	if (!AggregatedRecord) {
		if (debug) {
			console.log("Loading Protocol Buffer Model from " + protofile);
		}

		// create the builder from the proto file
		var builder = ProtoBuf.loadProtoFile(__dirname + "/" + protofile);
		AggregatedRecord = builder.build(kplConfig[useKplVersion].messageName);
	}
};

module.exports = MessageAggregator;

// function to calculate the worst case size of a provided record after encoding
function calculateSize(record) {
	// worst case size is that each partition and explicit hash key is unique
	// 9 bytes overhead (3 bytes per encoded item)
	var newBytes = Buffer.byteLength(record.PartitionKey)
			+ Buffer.byteLength(record.ExplicitHashKey)
			+ Buffer.byteLength(new Buffer(record.Data, 'base64')) + 9;

	return newBytes;
};

// function to compute the required hash key form a set of records
function getHashKey(records) {
	if (debug)
		console.log("Calculating Hash Key from " + records.length + " Records");

	// calculate the explicit hash key
	// as the first record's ExcplicitHashKey or PartitionKey
	return records[0].ExplicitHashKey ? records[0].ExplicitHashKey
			: records[0].PartitionKey;
};

// function which encodes the provided records
function flush(records, onReadyCallback) {
	if (debug)
		console.log("Flushing " + records.length + " records.");

	var hashKey = getHashKey(records);

	aggregationUtils.aggregate(records, function(err, encoded) {
		if (err) {
			onReadyCallback(err);
		} else {

			// call the provided callback
			onReadyCallback(undefined, {
				PartitionKey : 'a',
				ExplicitHashKey : hashKey,
				Data : encoded
			});
		}
	});
};

// constructor
function MessageAggregator(records) {
	loadBuilder();

	this.totalBytes = 0;
	this.putRecords = [];

	// initialise the current state with the provided records
	if (records) {
		this.totalBytes = calculateSize(records);
		this.putRecords.push(records);
	}
};

// method to force a flush of the current inflight records
MessageAggregator.prototype.flushBufferedRecords = function(onReadyCallback) {
	flush(this.putRecords, onReadyCallback);
};

// method to aggregate a set of records
MessageAggregator.prototype.aggregateRecords = function(records, forceFlush,
		afterRecordsCallback, onReadyCallback) {
	var self = this;

	records.map(function(record) {
		var newBytes = calculateSize(record);

		if (debug) {
			console.log("Current Pending Size: " + self.putRecords.length
					+ " records, " + self.totalBytes + " bytes");
			console.log("Next: " + newBytes + " bytes");
		}

		// if the size of these records would push us over the limit, then
		// encode the current set
		if ((self.totalBytes + newBytes) > KINESIS_MAX_PAYLOAD_BYTES
				|| forceFlush === true) {
			// flush with a copy of the current inflight records
			flush(_.clone(self.putRecords), onReadyCallback);

			// total size tracked is now the size of the current record
			self.totalBytes = newBytes;

			// current inflight becomes just this record
			self.putRecords = [ record ];
		} else {
			// the current set of records is still within the kinesis max
			// payload size so increment inflight/total bytes
			self.putRecords.push(record);
			self.totalBytes = self.totalBytes + newBytes;
		}
	});

	// done - call the afterRecordsCallback
	if (afterRecordsCallback)
		afterRecordsCallback();
};